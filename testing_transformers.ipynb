{
	"cells": [
		{
			"cell_type": "code",
			"execution_count": 1,
			"metadata": {},
			"outputs": [],
			"source": [
				"import torch\n",
				"import numpy as np\n",
				"import datasets\n",
				"\n",
				"from transformers import (\n",
				"    AutoModelForSeq2SeqLM,\n",
				"    AutoTokenizer,\n",
				"    Seq2SeqTrainingArguments,\n",
				"    Seq2SeqTrainer,\n",
				"    DataCollatorForSeq2Seq,\n",
				"    EarlyStoppingCallback\n",
				")\n",
				"\n",
				"# from tabulate import tabulate\n",
				"import nltk\n",
				"from datetime import datetime\n",
				"import os"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Using device: cuda\n"
					]
				}
			],
			"source": [
				"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
				"print(f\"Using device: {device}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 3,
			"metadata": {},
			"outputs": [],
			"source": [
				"os.environ[\"trust_remote_code\"] = \"True\""
			]
		},
		{
			"cell_type": "code",
			"execution_count": 4,
			"metadata": {},
			"outputs": [],
			"source": [
				"testing_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test[:2000]\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 5,
			"metadata": {},
			"outputs": [],
			"source": [
				"def flatten(example):\n",
				"    return {\n",
				"        \"document\": example[\"article\"],\n",
				"        \"summary\": example[\"highlights\"],\n",
				"    }\n",
				"\n",
				"def listToSamples(example):\n",
				"    result = {\"document\": example[\"document\"], \"summary\": example[\"summary\"]}\n",
				"    return result"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 6,
			"metadata": {},
			"outputs": [],
			"source": [
				"testing_dataset = testing_data.map(flatten)\n",
				"testing_dataset = testing_dataset.map(listToSamples)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 7,
			"metadata": {},
			"outputs": [],
			"source": [
				"def preprocess(batch, tokenizer, max_source_length, max_target_length):\n",
				"    source, target = batch[\"document\"], batch[\"summary\"]\n",
				"    source_tokenized = tokenizer(\n",
				"        source, padding=\"max_length\", truncation=True, max_length=max_source_length\n",
				"    )\n",
				"    target_tokenized = tokenizer(\n",
				"        target, padding=\"max_length\", truncation=True, max_length=max_target_length\n",
				"    )\n",
				"\n",
				"    batch = {k: v for k, v in source_tokenized.items()}\n",
				"    # Ignore padding in the loss\n",
				"    batch[\"labels\"] = [\n",
				"        [-100 if token == tokenizer.pad_token_id else token for token in l]\n",
				"        for l in target_tokenized[\"input_ids\"]\n",
				"    ]\n",
				"    return batch"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 8,
			"metadata": {},
			"outputs": [
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"c:\\Users\\ahmed\\anaconda3\\envs\\text_summarizer\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
						"  warnings.warn(\n"
					]
				}
			],
			"source": [
				"from transformers import pipeline\n",
				"# pipeline = pipeline(\"text2text-generation\", model=\"ahmeddsakrr/text_summarizer_t5\", tokenizer=\"t5-small\", device=\"cuda:0\", temperature=1)\n",
				"# pipeline = pipeline(\"text2text-generation\", model=\"ahmeddsakrr/text_summarizer_bart\", tokenizer=\"facebook/bart-base\", device=\"cuda:0\", temperature=1)\n",
				"pipeline = pipeline(\"text2text-generation\", model=\"ahmeddsakrr/text_summarizer_pegasus\", tokenizer=\"google/pegasus-xsum\", device=\"cuda:0\",temperature=1)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 9,
			"metadata": {},
			"outputs": [],
			"source": [
				"model = pipeline.model\n",
				"tokenizer = pipeline.tokenizer"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 10,
			"metadata": {},
			"outputs": [],
			"source": [
				"testing_data = testing_dataset.map(\n",
				"    lambda batch: preprocess(\n",
				"        batch, tokenizer, 512, 128\n",
				"    ),\n",
				"    batched=True,\n",
				"    remove_columns=testing_dataset.column_names,\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 11,
			"metadata": {},
			"outputs": [
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"C:\\Users\\ahmed\\AppData\\Local\\Temp\\ipykernel_19216\\682270908.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
						"  metric = datasets.load_metric(\"rouge\")\n",
						"c:\\Users\\ahmed\\anaconda3\\envs\\text_summarizer\\Lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/rouge/rouge.py\n",
						"You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
						"Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
						"  warnings.warn(\n"
					]
				}
			],
			"source": [
				"nltk.download(\"punkt\", quiet=True)\n",
				"\n",
				"metric = datasets.load_metric(\"rouge\")\n",
				"\n",
				"\n",
				"def postprocess_text(preds, labels):\n",
				"    preds = [pred.strip() for pred in preds]\n",
				"    labels = [label.strip() for label in labels]\n",
				"\n",
				"    # rougeLSum expects newline after each sentence\n",
				"    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
				"    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
				"\n",
				"    return preds, labels"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 12,
			"metadata": {},
			"outputs": [],
			"source": [
				"def compute_metrics(eval_preds):\n",
				"    preds, labels = eval_preds\n",
				"    if isinstance(preds, tuple):\n",
				"        preds = preds[0]\n",
				"    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
				"    # Replace -100 in the labels as we can't decode them.\n",
				"    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
				"    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
				"\n",
				"    # Some simple post-processing\n",
				"    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
				"\n",
				"    result = metric.compute(\n",
				"        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
				"    )\n",
				"    # Extract a few results from ROUGE\n",
				"    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
				"\n",
				"    prediction_lens = [\n",
				"        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
				"    ]\n",
				"    result[\"gen_len\"] = np.mean(prediction_lens)\n",
				"    result = {k: round(v, 4) for k, v in result.items()}\n",
				"    return result"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 13,
			"metadata": {},
			"outputs": [
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"c:\\Users\\ahmed\\anaconda3\\envs\\text_summarizer\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
						"  warnings.warn(\n"
					]
				}
			],
			"source": [
				"training_args = Seq2SeqTrainingArguments(\n",
				"    output_dir=\"results\",\n",
				"    num_train_epochs=5,  \n",
				"    do_train=True,\n",
				"    do_eval=True,\n",
				"    per_device_train_batch_size=4,  # demo\n",
				"    per_device_eval_batch_size=4,\n",
				"    learning_rate=3e-05,\n",
				"    warmup_steps=500,\n",
				"    weight_decay=0.1,\n",
				"    label_smoothing_factor=0.1,\n",
				"    predict_with_generate=True,\n",
				"    logging_dir=\"logs\",\n",
				"    logging_steps=50,\n",
				"    save_total_limit=3,\n",
				"    load_best_model_at_end=True,  # Load the best model at the end of training\n",
				"    metric_for_best_model=\"eval_loss\",  # Use evaluation loss to determine the best model\n",
				"    greater_is_better=False,  # Lower evaluation loss indicates a better model\n",
				"    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
				"    save_strategy=\"epoch\",        # Save at the end of each epoch\n",
				")\n",
				"\n",
				"data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
				"\n",
				"trainer = Seq2SeqTrainer(\n",
				"    model=model,\n",
				"    args=training_args,\n",
				"    data_collator=data_collator,\n",
				"    tokenizer=tokenizer,\n",
				"    compute_metrics=compute_metrics,\n",
				"    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 14,
			"metadata": {},
			"outputs": [
				{
					"data": {
						"text/plain": [
							"PegasusForConditionalGeneration(\n",
							"  (model): PegasusModel(\n",
							"    (shared): Embedding(96103, 1024, padding_idx=0)\n",
							"    (encoder): PegasusEncoder(\n",
							"      (embed_tokens): Embedding(96103, 1024, padding_idx=0)\n",
							"      (embed_positions): PegasusSinusoidalPositionalEmbedding(512, 1024)\n",
							"      (layers): ModuleList(\n",
							"        (0-15): 16 x PegasusEncoderLayer(\n",
							"          (self_attn): PegasusAttention(\n",
							"            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
							"            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
							"            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
							"            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
							"          )\n",
							"          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
							"          (activation_fn): ReLU()\n",
							"          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
							"          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
							"          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
							"        )\n",
							"      )\n",
							"      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
							"    )\n",
							"    (decoder): PegasusDecoder(\n",
							"      (embed_tokens): Embedding(96103, 1024, padding_idx=0)\n",
							"      (embed_positions): PegasusSinusoidalPositionalEmbedding(512, 1024)\n",
							"      (layers): ModuleList(\n",
							"        (0-15): 16 x PegasusDecoderLayer(\n",
							"          (self_attn): PegasusAttention(\n",
							"            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
							"            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
							"            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
							"            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
							"          )\n",
							"          (activation_fn): ReLU()\n",
							"          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
							"          (encoder_attn): PegasusAttention(\n",
							"            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
							"            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
							"            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
							"            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
							"          )\n",
							"          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
							"          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
							"          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
							"          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
							"        )\n",
							"      )\n",
							"      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
							"    )\n",
							"  )\n",
							"  (lm_head): Linear(in_features=1024, out_features=96103, bias=False)\n",
							")"
						]
					},
					"execution_count": 14,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"model.eval()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 29,
			"metadata": {},
			"outputs": [
				{
					"data": {
						"application/vnd.jupyter.widget-view+json": {
							"model_id": "967fc6537dc84a4d93f73feab312b15a",
							"version_major": 2,
							"version_minor": 0
						},
						"text/plain": [
							"  0%|          | 0/2873 [00:00<?, ?it/s]"
						]
					},
					"metadata": {},
					"output_type": "display_data"
				},
				{
					"data": {
						"text/plain": [
							"PredictionOutput(predictions=array([[    0,    37, 10748, ...,     3,     5,    37],\n",
							"       [    0,    37,    23, ...,   451,    47,   435],\n",
							"       [    0,  1290,  1483, ...,   118,   412,     5],\n",
							"       ...,\n",
							"       [    0, 11859,  6424, ...,    12, 10755,   112],\n",
							"       [    0,  2184,  1916, ...,   145,   192,    18],\n",
							"       [    0,     3,  8365, ...,    31,  7248,  2976]], dtype=int64), label_ids=array([[19428,  1527,     8, ...,  -100,  -100,  -100],\n",
							"       [   37,    23,     9, ...,  -100,  -100,  -100],\n",
							"       [ 1290,  1483, 11374, ...,  -100,  -100,  -100],\n",
							"       ...,\n",
							"       [ 6424,    63,    47, ...,  -100,  -100,  -100],\n",
							"       [ 2184,  1916,    72, ...,  -100,  -100,  -100],\n",
							"       [    3,  8365,   302, ...,    31,     7,     1]], dtype=int64), metrics={'test_loss': 0.78, 'test_rouge1': 0.6, 'test_rouge2': 0.52, 'test_rougeL': 0.64, 'test_runtime': 991.9659, 'test_samples_per_second': 11.583, 'test_steps_per_second': 2.896})"
						]
					},
					"execution_count": 29,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"# used for t5\n",
				"trainer.predict(testing_data)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 37,
			"metadata": {},
			"outputs": [
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"c:\\Users\\ahmed\\anaconda3\\envs\\text_summarizer\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:597: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
						"  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
						"c:\\Users\\ahmed\\anaconda3\\envs\\text_summarizer\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
						"  warnings.warn(\n"
					]
				},
				{
					"data": {
						"application/vnd.jupyter.widget-view+json": {
							"model_id": "6eb972408af94cccb10a25b79224610a",
							"version_major": 2,
							"version_minor": 0
						},
						"text/plain": [
							"  0%|          | 0/2873 [00:00<?, ?it/s]"
						]
					},
					"metadata": {},
					"output_type": "display_data"
				},
				{
					"data": {
						"text/plain": [
							"PredictionOutput(predictions=array([[    2,     0,   133, ..., 50118,   133,     2],\n",
							"       [    2,     0,   133, ...,    20,   493,     2],\n",
							"       [    2,     0, 29880, ...,  7076,     7,     2],\n",
							"       ...,\n",
							"       [    2,     0, 36128, ...,    12,   180,     2],\n",
							"       [    2,     0, 20770, ...,    80,    12,     2],\n",
							"       [    2,     0, 25441, ...,    71,  3357,     2]], dtype=int64), label_ids=array([[    0, 31339,  4128, ...,  -100,  -100,  -100],\n",
							"       [    0,   133,   493, ...,  -100,  -100,  -100],\n",
							"       [    0, 29880, 41007, ...,  -100,  -100,  -100],\n",
							"       ...,\n",
							"       [    0, 31574,   219, ...,  -100,  -100,  -100],\n",
							"       [    0, 20770,  1088, ...,  -100,  -100,  -100],\n",
							"       [    0, 25441,   687, ...,    11,  2920,     2]], dtype=int64), metrics={'test_loss': 0.81, 'test_rouge1': 0.61, 'test_rouge2': 0.58, 'test_rougeL': 0.63, 'test_runtime': 1576.3842, 'test_samples_per_second': 7.289, 'test_steps_per_second': 1.823})"
						]
					},
					"execution_count": 37,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"# used for bart\n",
				"trainer.predict(testing_data)   "
			]
		},
		{
			"cell_type": "code",
			"execution_count": 15,
			"metadata": {},
			"outputs": [
				{
					"data": {
						"application/vnd.jupyter.widget-view+json": {
							"model_id": "09faab85ee374c42a3f354fe9ce8ad9c",
							"version_major": 2,
							"version_minor": 0
						},
						"text/plain": [
							"  0%|          | 0/500 [00:00<?, ?it/s]"
						]
					},
					"metadata": {},
					"output_type": "display_data"
				},
				{
					"data": {
						"text/plain": [
							"PredictionOutput(predictions=array([[    0,   139, 18336, ...,     0,     0,     0],\n",
							"       [    0,   139,  2396, ...,   114, 11494,     1],\n",
							"       [    0, 26101,  6830, ...,     0,     0,     0],\n",
							"       ...,\n",
							"       [    0,  5420,   672, ...,     0,     0,     0],\n",
							"       [    0, 15254, 15960, ...,     0,     0,     0],\n",
							"       [    0, 11140, 10379, ...,   142, 11828,     1]], dtype=int64), label_ids=array([[10945,  1106,   109, ...,  -100,  -100,  -100],\n",
							"       [  139,  2396,   108, ...,  -100,  -100,  -100],\n",
							"       [26101,  6830,   252, ...,  -100,  -100,  -100],\n",
							"       ...,\n",
							"       [20313, 88541,  3999, ...,  -100,  -100,  -100],\n",
							"       [58869,   547,   109, ...,  -100,  -100,  -100],\n",
							"       [11140, 10379, 46657, ...,  -100,  -100,  -100]], dtype=int64), metrics={'test_loss': 0.65, 'test_rouge1': 0.67, 'test_rouge2': 0.53, 'test_rougeL': 0.57, 'test_runtime': 8415.3963, 'test_samples_per_second': 0.238, 'test_steps_per_second': 0.059})"
						]
					},
					"execution_count": 15,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"# used for pegasus\n",
				"trainer.predict(testing_data)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": []
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.12.4"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}
